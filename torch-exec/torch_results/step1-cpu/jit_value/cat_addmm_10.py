import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def forward(self, x):
        s1 = torch.addmm(x, x, x, beta=0.0, alpha=0.0)
        s2 = torch.addmm(x, x, x, beta=0.0, alpha=0.0)
        return torch.cat((s1, s2), -1)


func = Model().to('cpu')


x = torch.randn(10, 10)

test_inputs = [x]

# JIT_VALUE
'''
direct:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])

jit:
tensor([[ 1.1736, -2.9404,  4.0983, -2.9748,  1.9024,  1.3500,  1.4769,  1.2453,
          1.2108, -1.0740,  1.1736, -2.9404,  4.0983, -2.9748,  1.9024,  1.3500,
          1.4769,  1.2453,  1.2108, -1.0740],
        [ 7.5222, -0.2664,  2.6052, -2.5397, -2.3328,  3.2527, -1.1243,  2.4929,
          9.5750,  2.3376,  7.5222, -0.2664,  2.6052, -2.5397, -2.3328,  3.2527,
         -1.1243,  2.4929,  9.5750,  2.3376],
        [-6.7676, -5.0928,  0.2905, -4.4021, -3.5749, -0.8038,  3.4293, -0.0461,
          0.6051, -2.2609, -6.7676, -5.0928,  0.2905, -4.4021, -3.5749, -0.8038,
          3.4293, -0.0461,  0.6051, -2.2609],
        [-2.9430, -4.2967,  2.9977, -0.4795, -0.0895, -5.0434,  1.3235,  2.4534,
         -0.0655, -0.6928, -2.9430, -4.2967,  2.9977, -0.4795, -0.0895, -5.0434,
          1.3235,  2.4534, -0.0655, -0.6928],
        [-4.3331, -0.7452,  1.8378, -2.7892,  4.3837, -6.8189,  1.5378,  4.2062,
         -3.0552,  1.9732, -4.3331, -0.7452,  1.8378, -2.7892,  4.3837, -6.8189,
          1.5378,  4.2062, -3.0552,  1.9732],
        [ 4.5139, -2.0795,  3.1595,  0.2822, -4.4473, 12.7791,  0.7335, -4.6124,
          4.5533, -2.0789,  4.5139, -2.0795,  3.1595,  0.2822, -4.4473, 12.7791,
          0.7335, -4.6124,  4.5533, -2.0789],
        [ 0.3513,  1.6257, -5.0264, -2.5421,  0.8680, -2.6476,  2.6330,  0.3674,
         -4.1578,  2.3587,  0.3513,  1.6257, -5.0264, -2.5421,  0.8680, -2.6476,
          2.6330,  0.3674, -4.1578,  2.3587],
        [-1.2717,  2.1457, -3.8430,  1.7363,  2.4354, -7.4215, -0.2361,  1.6511,
         -3.0277, -0.1781, -1.2717,  2.1457, -3.8430,  1.7363,  2.4354, -7.4215,
         -0.2361,  1.6511, -3.0277, -0.1781],
        [-1.9616, -2.7230, -2.6417, -3.7982, -3.9700, -2.9635,  3.0066,  0.9734,
          0.4294,  1.4426, -1.9616, -2.7230, -2.6417, -3.7982, -3.9700, -2.9635,
          3.0066,  0.9734,  0.4294,  1.4426],
        [-3.2125, -2.6898,  1.8350, -1.5349, -1.9978, -4.5201,  1.8294,  2.6021,
         -0.0787,  3.0036, -3.2125, -2.6898,  1.8350, -1.5349, -1.9978, -4.5201,
          1.8294,  2.6021, -0.0787,  3.0036]])
'''